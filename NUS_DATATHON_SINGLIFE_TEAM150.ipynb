{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The cell below is for you to keep track of the libraries used and install those libraries quickly\n",
    "##### Ensure that the proper library names are used and the syntax of `%pip install PACKAGE_NAME` is followed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The filename, directory name, or volume label syntax is incorrect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The filename, directory name, or volume label syntax is incorrect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The filename, directory name, or volume label syntax is incorrect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The filename, directory name, or volume label syntax is incorrect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The filename, directory name, or volume label syntax is incorrect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The filename, directory name, or volume label syntax is incorrect.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "%pip install pyarrow\n",
    "%pip install numpy\n",
    "%pip install scikit-learn\n",
    "%pip install imbalanced-learn\n",
    "%pip install matplotlib\n",
    "# add commented pip installation lines for packages used as shown above for ease of testing\n",
    "# the line should be of the format %pip install PACKAGE_NAME "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DO NOT CHANGE** the filepath variable\n",
    "##### Instead, create a folder named 'data' in your current working directory and \n",
    "##### have the .parquet file inside that. A relative path *must* be used when loading data into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can have as many cells as you want for code\n",
    "import pandas as pd\n",
    "filepath = \"./data/catB_train.parquet\" \n",
    "# the initialised filepath MUST be a relative path to a folder named data that contains the parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ALL** Code for machine learning and dataset analysis should be entered below. \n",
    "##### Ensure that your code is clear and readable.\n",
    "##### Comments and Markdown notes are advised to direct attention to pieces of code you deem useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing and KNN Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23088\\73632724.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#importing data into dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./data/catB_train.parquet\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Convert target col to 0 or 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"f_purchase_lh\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"f_purchase_lh\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[0;32m    652\u001b[0m     \"\"\"\n\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 654\u001b[1;33m     \u001b[0mimpl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    655\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    656\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0muse_nullable_dtypes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_default\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mget_engine\u001b[1;34m(engine)\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0merror_msgs\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"\\n - \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         raise ImportError(\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;34m\"Unable to find a usable engine; \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;34m\"tried using: 'pyarrow', 'fastparquet'.\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "#importing data into dataframe\n",
    "df = pd.read_parquet(\"./data/catB_train.parquet\")\n",
    "\n",
    "# Convert target col to 0 or 1\n",
    "df[\"f_purchase_lh\"] = df[\"f_purchase_lh\"].fillna(0)\n",
    "\n",
    "# Identify numeric columns\n",
    "numeric_cols = df.select_dtypes(include=[\"int64\", \"float64\"])\n",
    "\n",
    "# Create a mask of non-NA values\n",
    "non_na_mask = numeric_cols.notna()\n",
    "\n",
    "# Initialize KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "\n",
    "#KNN imputation on non-NA values\n",
    "imputed_non_na = imputer.fit_transform(numeric_cols[non_na_mask])\n",
    "imputed_df = pd.DataFrame(imputed_non_na, columns=numeric_cols.columns, index=numeric_cols.index)\n",
    "\n",
    "# Replace NA values with imputed values from nearest non-NA neighbors\n",
    "for col in numeric_cols.columns:\n",
    "    df[col].fillna(imputed_df[col], inplace=True)\n",
    "\n",
    "\n",
    "# Display DataFrame after imputation\n",
    "#print(df)\n",
    "\n",
    "# Drop non-numeric cols\n",
    "non_numeric_cols = df.select_dtypes(include=[\"string\", \"object\"]).columns\n",
    "df = df.drop(columns=non_numeric_cols)\n",
    "\n",
    "y = df[\"f_purchase_lh\"]\n",
    "X = df.drop(columns=['f_purchase_lh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 17992 entries, 19550 to 15795\n",
      "Data columns (total 90 columns):\n",
      " #   Column                             Non-Null Count  Dtype  \n",
      "---  ------                             --------------  -----  \n",
      " 0   flg_substandard                    16196 non-null  float64\n",
      " 1   flg_is_borderline_standard         16196 non-null  float64\n",
      " 2   flg_is_revised_term                16196 non-null  float64\n",
      " 3   flg_is_rental_flat                 16196 non-null  float64\n",
      " 4   flg_has_health_claim               16196 non-null  float64\n",
      " 5   flg_has_life_claim                 16196 non-null  float64\n",
      " 6   flg_gi_claim                       16196 non-null  float64\n",
      " 7   flg_is_proposal                    16196 non-null  float64\n",
      " 8   flg_with_preauthorisation          16196 non-null  float64\n",
      " 9   flg_is_returned_mail               16196 non-null  float64\n",
      " 10  is_consent_to_mail                 16196 non-null  float64\n",
      " 11  is_consent_to_email                16196 non-null  float64\n",
      " 12  is_consent_to_call                 16196 non-null  float64\n",
      " 13  is_consent_to_sms                  16196 non-null  float64\n",
      " 14  is_valid_dm                        16196 non-null  float64\n",
      " 15  is_valid_email                     16196 non-null  float64\n",
      " 16  is_housewife_retiree               16196 non-null  float64\n",
      " 17  is_sg_pr                           16196 non-null  float64\n",
      " 18  is_class_1_2                       16196 non-null  float64\n",
      " 19  is_dependent_in_at_least_1_policy  16196 non-null  float64\n",
      " 20  f_ever_declined_la                 16196 non-null  float64\n",
      " 21  hh_size                            16196 non-null  float64\n",
      " 22  n_months_last_bought_products      16196 non-null  float64\n",
      " 23  flg_latest_being_lapse             16196 non-null  float64\n",
      " 24  flg_latest_being_cancel            16196 non-null  float64\n",
      " 25  recency_lapse                      16196 non-null  float64\n",
      " 26  recency_cancel                     16196 non-null  float64\n",
      " 27  tot_inforce_pols                   16196 non-null  float64\n",
      " 28  tot_cancel_pols                    16196 non-null  float64\n",
      " 29  f_hold_839f8a                      16196 non-null  float64\n",
      " 30  f_hold_e22a6a                      16196 non-null  float64\n",
      " 31  f_hold_d0adeb                      16196 non-null  float64\n",
      " 32  f_hold_c4bda5                      16196 non-null  float64\n",
      " 33  f_hold_ltc                         16196 non-null  float64\n",
      " 34  f_hold_507c37                      16196 non-null  float64\n",
      " 35  f_hold_gi                          16196 non-null  float64\n",
      " 36  f_ever_bought_839f8a               16196 non-null  float64\n",
      " 37  f_ever_bought_e22a6a               16196 non-null  float64\n",
      " 38  f_ever_bought_d0adeb               16196 non-null  float64\n",
      " 39  f_ever_bought_c4bda5               16196 non-null  float64\n",
      " 40  f_ever_bought_ltc                  16196 non-null  float64\n",
      " 41  f_ever_bought_507c37               16196 non-null  float64\n",
      " 42  f_ever_bought_gi                   16196 non-null  float64\n",
      " 43  f_ever_bought_ltc_1280bf           16196 non-null  float64\n",
      " 44  f_ever_bought_grp_6fc3e6           16196 non-null  float64\n",
      " 45  f_ever_bought_grp_de05ae           16196 non-null  float64\n",
      " 46  f_ever_bought_inv_dcd836           16196 non-null  float64\n",
      " 47  f_ever_bought_grp_945b5a           16196 non-null  float64\n",
      " 48  f_ever_bought_grp_6a5788           16196 non-null  float64\n",
      " 49  f_ever_bought_ltc_43b9d5           16196 non-null  float64\n",
      " 50  f_ever_bought_grp_9cdedf           16196 non-null  float64\n",
      " 51  f_ever_bought_lh_d0adeb            16196 non-null  float64\n",
      " 52  f_ever_bought_grp_1581d7           16196 non-null  float64\n",
      " 53  f_ever_bought_grp_22decf           16196 non-null  float64\n",
      " 54  f_ever_bought_lh_507c37            16196 non-null  float64\n",
      " 55  f_ever_bought_lh_839f8a            16196 non-null  float64\n",
      " 56  f_ever_bought_inv_e9f316           16196 non-null  float64\n",
      " 57  f_ever_bought_grp_caa6ff           16196 non-null  float64\n",
      " 58  f_ever_bought_grp_fd3bfb           16196 non-null  float64\n",
      " 59  f_ever_bought_lh_e22a6a            16196 non-null  float64\n",
      " 60  f_ever_bought_grp_70e1dd           16196 non-null  float64\n",
      " 61  f_ever_bought_grp_e04c3a           16196 non-null  float64\n",
      " 62  f_ever_bought_grp_fe5fb8           16196 non-null  float64\n",
      " 63  f_ever_bought_grp_94baec           16196 non-null  float64\n",
      " 64  f_ever_bought_grp_e91421           16196 non-null  float64\n",
      " 65  f_ever_bought_lh_f852af            16196 non-null  float64\n",
      " 66  f_ever_bought_lh_947b15            16196 non-null  float64\n",
      " 67  f_ever_bought_32c74c               16196 non-null  float64\n",
      " 68  f_elx                              16196 non-null  float64\n",
      " 69  f_mindef_mha                       16196 non-null  float64\n",
      " 70  f_retail                           16196 non-null  float64\n",
      " 71  flg_affconnect_show_interest_ever  16196 non-null  float64\n",
      " 72  flg_affconnect_ready_to_buy_ever   16196 non-null  float64\n",
      " 73  flg_affconnect_lapse_ever          16196 non-null  float64\n",
      " 74  affcon_visit_days                  16196 non-null  float64\n",
      " 75  n_months_since_visit_affcon        16196 non-null  float64\n",
      " 76  clmcon_visit_days                  16196 non-null  float64\n",
      " 77  recency_clmcon                     16196 non-null  float64\n",
      " 78  recency_clmcon_regis               16196 non-null  float64\n",
      " 79  recency_hlthclaim                  16196 non-null  float64\n",
      " 80  hlthclaim_cnt_success              16196 non-null  float64\n",
      " 81  recency_hlthclaim_success          16196 non-null  float64\n",
      " 82  hlthclaim_cnt_unsuccess            16196 non-null  float64\n",
      " 83  recency_hlthclaim_unsuccess        16196 non-null  float64\n",
      " 84  flg_hlthclaim_839f8a_ever          16196 non-null  float64\n",
      " 85  recency_hlthclaim_839f8a           16196 non-null  float64\n",
      " 86  flg_hlthclaim_14cb37_ever          16196 non-null  float64\n",
      " 87  recency_hlthclaim_14cb37           16196 non-null  float64\n",
      " 88  recency_giclaim                    16196 non-null  float64\n",
      " 89  f_purchase_lh                      16196 non-null  float64\n",
      "dtypes: float64(90)\n",
      "memory usage: 12.5 MB\n"
     ]
    }
   ],
   "source": [
    "df.head()\n",
    "df.shape\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flg_substandard               0\n",
       "flg_is_borderline_standard    0\n",
       "flg_is_revised_term           0\n",
       "flg_is_rental_flat            0\n",
       "flg_has_health_claim          0\n",
       "                             ..\n",
       "recency_hlthclaim_839f8a      0\n",
       "flg_hlthclaim_14cb37_ever     0\n",
       "recency_hlthclaim_14cb37      0\n",
       "recency_giclaim               0\n",
       "f_purchase_lh                 0\n",
       "Length: 90, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE to balance the dataset\n",
    "### If you encounter ImportError, please run the following commands:\n",
    "#### pip uninstall scikit-learn\n",
    "#### pip install scikit-learn==1.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_MissingValues' from 'sklearn.utils._param_validation' (C:\\Users\\Hanrui\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE, RandomOverSampler\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      5\u001b[0m X_train, X_val, y_train, y_val \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\imblearn\\__init__.py:52\u001b[0m\n\u001b[0;32m     48\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartial import of imblearn during the build process.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     53\u001b[0m         combine,\n\u001b[0;32m     54\u001b[0m         ensemble,\n\u001b[0;32m     55\u001b[0m         exceptions,\n\u001b[0;32m     56\u001b[0m         metrics,\n\u001b[0;32m     57\u001b[0m         over_sampling,\n\u001b[0;32m     58\u001b[0m         pipeline,\n\u001b[0;32m     59\u001b[0m         tensorflow,\n\u001b[0;32m     60\u001b[0m         under_sampling,\n\u001b[0;32m     61\u001b[0m         utils,\n\u001b[0;32m     62\u001b[0m     )\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FunctionSampler\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\imblearn\\combine\\__init__.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"The :mod:`imblearn.combine` provides methods which combine\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mover-sampling and under-sampling.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_enn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTEENN\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_smote_tomek\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTETomek\n\u001b[0;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTEENN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMOTETomek\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\imblearn\\combine\\_smote_enn.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_X_y\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseSampler\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseOverSampler\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\imblearn\\base.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulticlass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_classification_targets\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_sampling_strategy, check_target_type\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArraysTransformer\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSamplerMixin\u001b[39;00m(BaseEstimator, metaclass\u001b[38;5;241m=\u001b[39mABCMeta):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\imblearn\\utils\\_param_validation.py:908\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate_valid_param  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m    907\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m--> 908\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    909\u001b[0m     HasMethods,\n\u001b[0;32m    910\u001b[0m     Hidden,\n\u001b[0;32m    911\u001b[0m     Interval,\n\u001b[0;32m    912\u001b[0m     Options,\n\u001b[0;32m    913\u001b[0m     StrOptions,\n\u001b[0;32m    914\u001b[0m     _ArrayLikes,\n\u001b[0;32m    915\u001b[0m     _Booleans,\n\u001b[0;32m    916\u001b[0m     _Callables,\n\u001b[0;32m    917\u001b[0m     _CVObjects,\n\u001b[0;32m    918\u001b[0m     _InstancesOf,\n\u001b[0;32m    919\u001b[0m     _IterablesNotString,\n\u001b[0;32m    920\u001b[0m     _MissingValues,\n\u001b[0;32m    921\u001b[0m     _NoneConstraint,\n\u001b[0;32m    922\u001b[0m     _PandasNAConstraint,\n\u001b[0;32m    923\u001b[0m     _RandomStates,\n\u001b[0;32m    924\u001b[0m     _SparseMatrices,\n\u001b[0;32m    925\u001b[0m     _VerboseHelper,\n\u001b[0;32m    926\u001b[0m     make_constraint,\n\u001b[0;32m    927\u001b[0m     validate_params,\n\u001b[0;32m    928\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_MissingValues' from 'sklearn.utils._param_validation' (C:\\Users\\Hanrui\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py)"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_base, X_test_base, y_train_base, y_test_base = train_test_split(X_base, y_base, test_size = 0.20, random_state = 42)\n",
    "\n",
    "def balancing(X,y,type):\n",
    "       '''\n",
    "       This function helps us to choose the type of resampling that we \n",
    "       want to do, functional abstraction, 1010 things :-)\n",
    "       '''\n",
    "       ros = RandomOverSampler(random_state=42)\n",
    "       smote = SMOTE(random_state=42)\n",
    "       if type == 'oversample':\n",
    "              return ros.fit_resample(X, y)\n",
    "       else:\n",
    "              return smote.fit_resample(X,y)\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)\n",
    "\n",
    "# Balance the training data \n",
    "X_train , y_train = balancing(X_train , y_train, 'smote')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cell below is **NOT** to be removed\n",
    "##### The function is to be amended so that it accepts the given input (dataframe) and returns the required output (list). \n",
    "##### It is recommended to test the function out prior to submission\n",
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "##### The hidden_data parsed into the function below will have the same layout columns wise as the dataset *SENT* to you\n",
    "##### Thus, ensure that steps taken to modify the initial dataset to fit into the model are also carried out in the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_hidden_data(hidden_data: pd.DataFrame) -> list:\n",
    "    '''DO NOT REMOVE THIS FUNCTION.\n",
    "\n",
    "The function accepts a dataframe as input and return an iterable (list)\n",
    "of binary classes as output.\n",
    "\n",
    "The function should be coded to test on hidden data\n",
    "and should include any preprocessing functions needed for your model to perform. \n",
    "    \n",
    "All relevant code MUST be included in this function.'''\n",
    "    result = [] \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cell to check testing_hidden_data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell should output a list of predictions.\n",
    "test_df = pd.read_parquet(filepath)\n",
    "test_df = test_df.drop(columns=[\"f_purchase_lh\"])\n",
    "print(testing_hidden_data(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please have the filename renamed and ensure that it can be run with the requirements above being met. All the best!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
